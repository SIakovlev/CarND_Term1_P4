{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Camera calibration step\n",
    "\n",
    "nx = 9\n",
    "ny = 6\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((ny*nx, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        \n",
    "        cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "        write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "        print(write_name)\n",
    "        #cv2.imwrite(write_name, img)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test undistortion on an image\n",
    "img = cv2.imread('camera_cal/calibration1.jpg')\n",
    "img_size = (img.shape[1], img.shape[0])\n",
    "# Do camera calibration given object points and image points\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None,None)\n",
    "\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "cv2.imwrite('test_undist.jpg',dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize undistortion\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(dst)\n",
    "ax2.set_title('Undistorted Image', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "dist_pickle = {}\n",
    "dist_pickle[\"mtx\"] = mtx\n",
    "dist_pickle[\"dist\"] = dist\n",
    "pickle.dump(dist_pickle, open( \"camera_cal/dist_pickle.p\", \"wb\" ) )\n",
    "print('Data is stored to the pickle file under ./camera_cal/dist_pickle.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading from pickle file is finished\n"
     ]
    }
   ],
   "source": [
    "# Load the camera calibration result from pickle file\n",
    "dist_pickle = pickle.load( open( \"camera_cal/dist_pickle.p\", \"rb\" ) )\n",
    "mtx = dist_pickle[\"mtx\"]\n",
    "dist = dist_pickle[\"dist\"]\n",
    "print('Data loading from pickle file is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/home/travis/miniconda/conda-bld/conda_1486587071158/work/opencv-3.1.0/modules/imgproc/src/undistort.cpp:191: error: (-215) dst.data != src.data in function undistort\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8133c4ad77c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Quick test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'camera_cal/calibration1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mundistort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /home/travis/miniconda/conda-bld/conda_1486587071158/work/opencv-3.1.0/modules/imgproc/src/undistort.cpp:191: error: (-215) dst.data != src.data in function undistort\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "img = cv2.imread('camera_cal/calibration1.jpg')\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax2.imshow(dst)\n",
    "ax2.set_title('Undistorted Image', fontsize=20)\n",
    "plt.savefig('report_images/calibration.jpg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Download an image\n",
    "image = cv2.imread('test_images/test5.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# Undistort it and show the result\n",
    "image_dst = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax2.imshow(image_dst)\n",
    "ax2.set_title('Undistorted Image', fontsize=20)\n",
    "plt.savefig('report_images/undistortion.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = np.float32([[0, 720], \n",
    "                  [575, 450],\n",
    "                  [705, 450],\n",
    "                  #[465, 500], \n",
    "                  #[815, 500], \n",
    "                  [1280, 720]])\n",
    "#dst = np.float32([[0, img_size[1]],\n",
    " #                 [0, 0],\n",
    "  #                [img_size[0], 0], \n",
    "   #               [img_size[0], img_size[1]]])\n",
    "dst = np.float32([[100, 720],\n",
    "                  [100, 0],\n",
    "                  [1280-100, 0], \n",
    "                  [1280-100, 720]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     16,
     19,
     23,
     26,
     29,
     32,
     39,
     46,
     53,
     60,
     63,
     75,
     82,
     93,
     105,
     120,
     135,
     138
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_mask(img, pts, color = (0, 255, 0)):\n",
    "    image = np.copy(img)\n",
    "    pts = np.array(pts, np.int32)\n",
    "    pts = pts.reshape((-1,1,2))\n",
    "    return cv2.polylines(image, [pts],True, color, lineType = cv2.LINE_AA, thickness = 2)\n",
    "\n",
    "def perspective(img, mode='f', scr=src, dst=dst):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "   \n",
    "    if mode == 'f':\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "    elif mode == 'inv':\n",
    "        M = cv2.getPerspectiveTransform(dst, src)\n",
    "        \n",
    "    return cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "def undistortion(img, mtx, dist):\n",
    "    return cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "def blur(img, k=5):\n",
    "    kernel_size = (k, k)\n",
    "    return cv2.GaussianBlur(img, kernel_size, 0)\n",
    "\n",
    "def channel(img, ch):\n",
    "    return img[:, :, ch]\n",
    "\n",
    "def bgr2rgb(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def rgb2gray(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def rgb2lab(img, ch=-1):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    if ch < 0:\n",
    "        return img\n",
    "    else:\n",
    "        return img[:, :, ch]\n",
    "\n",
    "def rgb2hls(img, ch=-1):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    if ch < 0:\n",
    "        return img\n",
    "    else:\n",
    "        return img[:, :, ch]\n",
    "\n",
    "def rgb2hsv(img, ch=-1):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    if ch < 0:\n",
    "        return img\n",
    "    else:\n",
    "        return img[:, :, ch]\n",
    "\n",
    "def rgb2yuv(img, ch=-1):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    if ch < 0:\n",
    "        return img\n",
    "    else:\n",
    "        return img[:, :, ch]\n",
    "\n",
    "def canny(img, thresh=(0, 255)):\n",
    "    return cv2.Canny(img, thresh[0], thresh[1])\n",
    "\n",
    "def sobel_abs(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    if orient == 'x':\n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "    elif orient == 'y':\n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "def channel_thresholding(img, ch, thresh=(0, 255)):\n",
    "    channel = img[:,:,ch]\n",
    "    binary_output = np.zeros_like(channel)\n",
    "    binary_output[(channel>thresh[0]) & (channel<=thresh[1])] = 1\n",
    "\n",
    "    return binary_output\n",
    "\n",
    "def plot(img, ch=-1, size=(5, 3), cmap=None, name=None):\n",
    "    plt.figure(figsize=size)\n",
    "    if ch < 0:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img[:, :, ch], cmap=cmap)\n",
    "    if name:\n",
    "        plt.savefig('report_images/' + name + '.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_many(imgs, cmaps=[], name=None):\n",
    "    n = len(imgs)\n",
    "    f, ax = plt.subplots(1, n, figsize=(n*10,5))\n",
    "    for i in range(n):\n",
    "        if not cmaps:\n",
    "            ax[i].imshow(imgs[i])\n",
    "        else:\n",
    "            ax[i].imshow(imgs[i], cmap = cmaps[i])\n",
    "    if name:\n",
    "        plt.savefig('report_images/' + name + '.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def yellow_select(img):    \n",
    "    lower = np.array([15,120,120], dtype=np.uint8)\n",
    "    upper = np.array([55,200,255], dtype=np.uint8)\n",
    "    \n",
    "    channel_h = img[:, :, 0]\n",
    "    channel_l = img[:, :, 1]\n",
    "    channel_s = img[:, :, 2]\n",
    "    \n",
    "    binary_output = np.zeros_like(img[:,:,0])\n",
    "    binary_output[((channel_h > lower[0]) & (channel_h <= upper[0])) \n",
    "                  & ((channel_l > lower[1]) & (channel_l <= upper[1])) \n",
    "                  & ((channel_s > lower[2]) & (channel_s <= upper[2]))] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "def white_select(img):    \n",
    "    lower = np.array([0,210,0], dtype=np.uint8)\n",
    "    upper = np.array([360,255,255], dtype=np.uint8)\n",
    "    \n",
    "    channel_h = img[:, :, 0]\n",
    "    channel_l = img[:, :, 1]\n",
    "    channel_s = img[:, :, 2]\n",
    "    \n",
    "    binary_output = np.zeros_like(img[:,:,0])\n",
    "    binary_output[((channel_h > lower[0]) & (channel_h <= upper[0])) \n",
    "                  & ((channel_l > lower[1]) & (channel_l <= upper[1])) \n",
    "                  & ((channel_s > lower[2]) & (channel_s <= upper[2]))] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "def weighted_img(initial_img, img, α=0.8, β=1., λ=0.):\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "def sobel_dir(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    \n",
    "    sobelx_abs = np.absolute(sobelx)\n",
    "    \n",
    "    ang_sobel = np.arctan2(np.absolute(sobely), sobelx_abs)    \n",
    "    binary_output = np.zeros_like(ang_sobel)\n",
    "    binary_output[(ang_sobel >= thresh[0]) & (ang_sobel <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv2.imread('test_images/test6.jpg')\n",
    "test_und = undistortion(img=bgr2rgb(test), mtx=mtx, dist=dist)\n",
    "combined1 = np.zeros_like(channel(test_und, ch=0))\n",
    "combined1[(yellow_select(rgb2hls(test_und)) == 1)] = 1\n",
    "\n",
    "combined2 = np.zeros_like(channel(test_und, ch=0))\n",
    "combined2[(yellow_select(rgb2hls(test_und)) == 1) | \n",
    "          (white_select(rgb2hls(test_und)) == 1) ] = 1\n",
    "\n",
    "combined3 = np.zeros_like(channel(test_und, ch=0))\n",
    "combined3[(yellow_select(rgb2hls(test_und)) == 1) |\n",
    "          (white_select(rgb2hls(test_und)) == 1) |\n",
    "          (sobel_abs(rgb2gray(test_und), thresh=(20, 100)) == 1)] = 1\n",
    "\n",
    "plot_many([test_und, combined1, combined2, combined3], cmaps=[None, 'gray', 'gray', 'gray'], name='thresh_bin_img')\n",
    "\n",
    "\n",
    "plot_many([perspective(test_und), \n",
    "           perspective(combined1), \n",
    "           perspective(combined2), \n",
    "           perspective(combined3)], cmaps=[None, 'gray', 'gray', 'gray'], name='thresh_bin_warp_img')\n",
    "\n",
    "plot_many([test_und, ], name='pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# for report\n",
    "img_pip, left_fit, right_fit = pipeline(test_und)\n",
    "ploty = np.linspace(0, 719, 720)\n",
    "left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(img_pip)\n",
    "plt.plot(left_fitx, ploty, color='yellow')\n",
    "plt.plot(right_fitx, ploty, color='yellow')\n",
    "plt.xlim(0, 1280)\n",
    "plt.ylim(720, 0)\n",
    "plt.savefig('report_images/pipeline.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "#test = bgr2rgb(cv2.imread('test_images/test5.jpg'))\n",
    "test_set = glob.glob('challenge_images/*.png')\n",
    "for sample in test_set:\n",
    "    test = cv2.imread(sample)\n",
    "    test_und = undistortion(img=bgr2rgb(test), mtx=mtx, dist=dist)\n",
    "\n",
    "    combined = np.zeros_like(channel(test, ch=0))\n",
    "    combined[(yellow_select(rgb2hls(test_und)) == 1) |\n",
    "            (white_select(rgb2hls(test_und)) == 1) ] = 1\n",
    "            #(sobel_abs(rgb2gray(test_und), thresh=(20, 100)) == 1) |\n",
    "            #(sobel_dir(rgb2hls(test_und, ch=2), thresh=(0.5, 1.3)) == 1)] = 1\n",
    "\n",
    "    plot_many([draw_mask(test_und, src), draw_mask(perspective(test_und), dst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "#test = cv2.imread('test_images/test2.jpg')\n",
    "#test = cv2.imread('challenge_images/0.63.png')\n",
    "test_set = glob.glob('test_images/*.jpg')\n",
    "#test_set = glob.glob('challenge_images/*.png')\n",
    "for i, sample in enumerate(test_set):\n",
    "    test = cv2.imread(sample)\n",
    "    test_und = undistortion(img=bgr2rgb(test), mtx=mtx, dist=dist)\n",
    "    test_x = sobel_abs(rgb2gray(test_und), thresh=(20, 100))\n",
    "    \n",
    "    test_yellow = yellow_select(rgb2hls(test_und))\n",
    "    test_white = white_select(rgb2hls(test_und))\n",
    "    plot_many([test_und, pipeline(test_und)], name = 'pipeline_test_images_'+str(i)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv2.imread('test_images/straight_lines1.jpg')\n",
    "test_und = undistortion(img=bgr2rgb(test), mtx=mtx, dist=dist)\n",
    "plot_many([draw_mask(test_und, src), draw_mask(perspective(test_und), dst)], name='region2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(img, mtx=mtx, dist=dist, visualisation = False):  \n",
    "    img = undistortion(img=img, mtx=mtx, dist=dist)\n",
    "\n",
    "    combined = np.zeros_like(channel(img, ch=0))\n",
    "    combined[(yellow_select(rgb2hls(img)) == 1) |\n",
    "             (white_select(rgb2hls(img)) == 1) |\n",
    "             (sobel_abs(rgb2gray(img), thresh=(20, 100)) == 1)] = 1\n",
    "    \n",
    "    combined_p = perspective(combined, 'f')\n",
    "    \n",
    "    b = np.mean(rgb2hsv(perspective(img), ch=2))\n",
    "    betha = 0.3/np.sqrt(1+0.3*(b-90)**2)\n",
    "    detections, lines_warp, left_fit, right_fit, R, offset = detect_lines_v2(combined_p, betha=betha)\n",
    "    \n",
    "    region_warp = draw_region(combined_p, left_fit, right_fit)\n",
    "    \n",
    "    region = perspective(region_warp, mode='inv')\n",
    "    lines = perspective(lines_warp, mode='inv')\n",
    "    \n",
    "    # Combine the result with the original image\n",
    "    img = cv2.addWeighted(img, 1, region, 0.3, 0)\n",
    "    \n",
    "    img = add_pic(img, detections)\n",
    "    \n",
    "    \n",
    "    if offset < 0:\n",
    "        offset_text = \"Offset to the right: {:0.2f} m\".format(offset)\n",
    "    else:\n",
    "        offset_text = \"Offset to the left: {:0.2f} m\".format(offset)\n",
    "    text = [\"Radius of curvature: {:0.2f} m\".format(R), offset_text, \"Betha: {:0.2f}\".format(betha)]\n",
    "    \n",
    "    img = info_table(img, text)\n",
    "    \n",
    "    img = weighted_img(lines, img)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lines detection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect lines\n",
    "#### Logic\n",
    "\n",
    "* Green cells define region of interest (ROI)\n",
    "* Convolution with the current image layer defines probability distribution of picking points from the ROI\n",
    "\n",
    "\n",
    "#### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     13,
     29,
     32
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional routines\n",
    "def add_pic(img, info_pic):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    info_size = 300\n",
    "    r = float(info_size / img_size[0])\n",
    "    dim = (info_size, int(img_size[1] * r))\n",
    "    info = cv2.resize(info_pic, dim)\n",
    "    \n",
    "    x_offset=y_offset=1\n",
    "    cv2.rectangle(img, (0, 0), (info_size + 2, int(img_size[1] * r + 2)), (0,255,0), thickness = 1, lineType = cv2.LINE_AA)\n",
    "    img[y_offset:y_offset+info.shape[0], x_offset:x_offset+info.shape[1]] = info\n",
    "    return img\n",
    "\n",
    "def info_table(img, text, size=(300, 100)):\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    length = size[0]\n",
    "    width = size[1]\n",
    "    \n",
    "    cv2.rectangle(img, (img_size[0]-length+1, 0+1), (img_size[0]-1, width-1), (0,0,0), thickness = -1, lineType = cv2.LINE_AA)\n",
    "    cv2.rectangle(img, (img_size[0]-length, 0), (img_size[0], width), (0,255,0), thickness = 1, lineType = cv2.LINE_AA)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    cv2.putText(img, text[0], (img_size[0]-length + 10, width-10), font, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.putText(img, text[1], (img_size[0]-length + 10, width-40), font, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.putText(img, text[2], (img_size[0]-length + 10, width-70), font, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "def bin2rgb(warped_binary):\n",
    "    return np.asarray(np.dstack((warped_binary, warped_binary, warped_binary))*255, dtype=np.uint8)\n",
    "\n",
    "def plot2nparray(vector, my_dpi = 96):    \n",
    "    fig = plt.figure(figsize=(300/my_dpi, 168/my_dpi), dpi=my_dpi)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(vector)\n",
    "    fig.canvas.draw()\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close()\n",
    "    return data\n",
    "\n",
    "def draw_region(warped, left_fit, right_fit):\n",
    "    img_x = warped.shape[1]\n",
    "    img_y = warped.shape[0]\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img_y-1, img_y )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    center_fit = (left_fit + right_fit)/2\n",
    "    center_fitx = center_fit[0]*ploty**2 + center_fit[1]*ploty + center_fit[2]\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_center = np.array([np.flipud(np.transpose(np.vstack([center_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_center))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (255, 0, 0))\n",
    "    \n",
    "    ## Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_center = np.array([np.transpose(np.vstack([center_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_center, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,0, 255))\n",
    "\n",
    "    return color_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_fit = np.zeros(3)\n",
    "right_fit = np.zeros(3)\n",
    "left_fit_cr = np.zeros(3)\n",
    "right_fit_cr = np.zeros(3)\n",
    "offset = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_lines_v2(image, nwindows = 9, window_width = 50, betha=0.1, alpha=0.9):\n",
    "    \n",
    "    # Bunch of global vars\n",
    "    global left_fit\n",
    "    global right_fit\n",
    "    global left_fit_cr\n",
    "    global right_fit_cr\n",
    "    global offset\n",
    "    \n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "    img_x = image.shape[1]\n",
    "    img_y = image.shape[0]\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    img_nonzero = image.nonzero()\n",
    "    nonzeroy = np.array(img_nonzero[0])\n",
    "    nonzerox = np.array(img_nonzero[1])\n",
    "    \n",
    "    window_height = np.int(img_y/nwindows)\n",
    "    center = int(img_x/2)\n",
    "    \n",
    "    # Create an output images to draw on and visualize the result\n",
    "    out_img = np.dstack((image, image, image))*255\n",
    "    lines_img = np.zeros_like(out_img)\n",
    "    \n",
    "    # Set parameters of windows\n",
    "    w = np.ones(window_width)\n",
    "    \n",
    "    #if all(v == 0 for v in conv_total):\n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(image[int(img_y/2):,:center], axis=0)\n",
    "    conv_l = np.convolve(w, l_sum, 'same')\n",
    "    l_distr = conv_l/np.max(conv_l)\n",
    "    leftx_base = np.argmax(l_distr)\n",
    "\n",
    "    r_sum = np.sum(image[int(img_y/2):,center:], axis=0)\n",
    "    conv_r = np.convolve(w, r_sum, 'same')\n",
    "    r_distr = conv_r/np.max(conv_r)\n",
    "    rightx_base = np.argmax(r_distr)+center\n",
    "    \n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    alpha = 0.9\n",
    "    #conv_image = np.zeros(img_x)\n",
    "    # Step through the windows one by one\n",
    "    for level in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img_y - (level+1)*window_height\n",
    "        win_y_high = img_y - level*window_height\n",
    "        win_xleft_low = max(0, leftx_current - margin)\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = min(rightx_current + margin, img_x)\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),\n",
    "        (0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),\n",
    "        (0,255,0), 2) \n",
    "\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            img_layer = image[win_y_low:win_y_high, win_xleft_low:win_xleft_high]\n",
    "            img_layer_hist = np.sum(img_layer, axis=0)\n",
    "            conv_left = np.convolve(w, img_layer_hist, 'same')\n",
    "            mean = np.argmax(conv_left) + win_xleft_low\n",
    "            leftx_current = np.int(alpha*mean + (1-alpha)*leftx_current)\n",
    "            \n",
    "        if len(good_right_inds) > minpix:\n",
    "            \n",
    "            img_layer = image[win_y_low:win_y_high, win_xright_low:win_xright_high]\n",
    "            img_layer_hist = np.sum(img_layer, axis=0)\n",
    "            conv_right = np.convolve(w, img_layer_hist, 'same')\n",
    "            mean = np.argmax(conv_right) + win_xright_low\n",
    "            rightx_current = np.int(alpha*mean + (1-alpha)*rightx_current)\n",
    "        \n",
    "        if level < 3:\n",
    "            offset = (1-alpha)*offset + alpha*(rightx_current - leftx_current - center)\n",
    "    \n",
    "    betha = 0.1\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = betha*np.polyfit(lefty, leftx, 2) + (1-betha)*left_fit\n",
    "    right_fit = betha*np.polyfit(righty, rightx, 2) + (1-betha)*right_fit\n",
    "\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    lines_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    lines_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    y_eval = img_y\n",
    "    \n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 24/img_y # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/750 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    #left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    #right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    ploty = np.linspace(0, img_y-1, num=img_y)\n",
    "    leftx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    rightx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    left_fit_cr = betha*np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2) + (1-betha)*left_fit_cr\n",
    "    right_fit_cr = betha*np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2) + (1-betha)*right_fit_cr\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    curvature = (left_curverad*len(lefty) + right_curverad*len(righty))/(len(lefty) + len(righty))\n",
    "    \n",
    "    offset_m = offset*xm_per_pix\n",
    "\n",
    "    return out_img, lines_img, left_fit, right_fit, curvature, offset_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "white_output = 'test1.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(pipeline) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract frames from video\n",
    "import os\n",
    "from moviepy.editor import *\n",
    "\n",
    "def extract_frames(movie, times, imgdir):\n",
    "    clip = VideoFileClip(movie)\n",
    "    for t in times:\n",
    "        imgpath = os.path.join(imgdir, '{}.png'.format(t))\n",
    "        clip.save_frame(imgpath, t)\n",
    "\n",
    "movie = \"challenge_video.mp4\"\n",
    "imgdir = \"challenge_images\"\n",
    "times = 4.3, 4.4\n",
    "\n",
    "extract_frames(movie, times, imgdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
